<!-- [![DDPM inversion](https://img.shields.io/badge/single%20image-generative%20model-yellow)](https://github.com/topics/single-image-generation) -->
[![Python 3.8](https://img.shields.io/badge/python-3.812+-blue)](https://www.python.org/downloads/release/python-38/)
[![torch](https://img.shields.io/badge/torch-2.0.0+-green)](https://pytorch.org/)


# DDPM inversion, CVPR 2024

[Project page](https://inbarhub.github.io/DDPM_inversion/) | [Arxiv](https://arxiv.org/abs/2304.06140) | [Supplementary materials](https://inbarhub.github.io/DDPM_inversion/resources/inversion_supp.pdf) | [Hugging Face Demo](https://huggingface.co/spaces/LinoyTsaban/edit_friendly_ddpm_inversion)
### Official pytorch implementation of the paper: <br>"An Edit Friendly DDPM Noise Space: Inversion and Manipulations"
#### Inbar Huberman-Spiegelglas, Vladimir Kulikov and Tomer Michaeli 
<br>

![](imgs/teaser.jpg)
Our inversion can be used for text-based **editing of real images**, either by itself or in combination with other editing methods.
Due to the stochastic nature of our method, we can generate **diverse outputs**, a feature that is not naturally available with methods relying on the DDIM inversion.

In this repository we support editing using our inversion, prompt-to-prompt (p2p)+our inversion, ddim or [p2p](https://github.com/google/prompt-to-prompt) (with ddim inversion).<br>
**our inversion**: our ddpm inversion followed by generating an image conditioned on the target prompt. 

**prompt-to-prompt (p2p) + our inversion**: p2p method using our ddpm inversion. 

**ddim**: ddim inversion followed by generating an image conditioned on the target prompt.

**p2p**: p2p method using ddim inversion (original paper).

## Table of Contents
* [Requirements](#Requirements)
* [Repository Structure](#Repository-Structure)
* [Algorithm Inputs and Parameters](#Algorithm-Inputs-and-Parameters)
* [Usage Example](#Usage-Example)

* [Citation](#Citation)

## Requirements 

```
python -m pip install -r requirements.txt
```
This code was tested with python 3.8 and torch 2.0.0. 

## Repository Structure 
```
â”œâ”€â”€ ddm_inversion - folder contains inversions in order to work on real images: ddim inversion as well as ddpm inversion (our method).
â”œâ”€â”€ example_images - folder of input images to be edited
â”œâ”€â”€ imgs - images used in this repository readme.md file
â”œâ”€â”€ prompt_to_prompt - p2p code
â”œâ”€â”€ main_run.py - main python file for real image editing
â””â”€â”€ test.yaml - yaml file contains images and prompts to test on
```

A folder named 'results' will be automatically created and all the results will be saved to this folder. We also add a timestamp to the saved images in this folder.

## Algorithm Inputs and Parameters
Method's inputs: 
```
init_img - the path to the input images
source_prompt - a prompt describing the input image
target_prompts - the edit prompt (creates several images if multiple prompts are given)
```
These three inputs are supplied through a YAML file (please use the provided 'test.yaml' file as a reference).

<br>
Method's parameters are:

```
skip - controlling the adherence to the input image
cfg_tar - classifier free guidance strengths
```
These two parameters have default values, as descibed in the paper.

## Usage Example 
```
python3 main_run.py --mode="our_inv" --dataset_yaml="test.yaml" --skip=36 --cfg_tar=15 
python3 main_run.py --mode="p2pinv" --dataset_yaml="test.yaml" --skip=12 --cfg_tar=9 

```
The ```mode``` argument can also be: ```ddim``` or ```p2p```.

In ```our_inv``` and ```p2pinv``` modes we suggest to play around with ```skip``` in the range [0,40] and ```cfg_tar``` in the range [7,18].

**p2pinv and p2p**:
Note that you can play with the cross-and self-attention via ```--xa``` and ```--sa``` arguments. We suggest to set them to (0.6,0.2) and (0.8,0.4) for p2pinv and p2p respectively.

**ddim and p2p**:
```skip``` is overwritten to be 0.

<!-- ## Create Your Own Editing with Our Method
(1) Add your image to /example_images. <br>
(2) Run ``main_run.py --mode="our_inv"``, choose ``skip`` and ``cfg_tar``. <br>

Example:
```
python3 main_run.py --skip=20 --cfg_tar=10 --img_name=gnochi_mirror --cfg_src='a cat is sitting next to a mirror' --cfg_tar='a drawing of a cat sitting next to a mirror'
```  -->

You can edit the test.yaml file to load your image and choose the desired prompts.

<!-- ## Sources 

The DDPM code was adapted from the following [pytorch implementation of DDPM](https://github.com/lucidrains/denoising-diffusion-pytorch). 

The modified CLIP model as well as most of the code in `./text2live_util/` directory was taken from the [official Text2live repository](https://github.com/omerbt/Text2LIVE).  -->
 
## Citation
If you use this code for your research, please cite our paper:
```
@inproceedings{huberman2024edit,
  title={An edit friendly {DDPM} noise space: Inversion and manipulations},
  author={Huberman-Spiegelglas, Inbar and Kulikov, Vladimir and Michaeli, Tomer},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12469--12478},
  year={2024}
}
```

# DDPMå›¾åƒèåˆä¸æ™ºèƒ½æ•°æ®æ‰©å¢

æœ¬é¡¹ç›®åœ¨DDPMå¯ç¼–è¾‘å™ªå£°ç©ºé—´ä¸­å®ç°äº†å¼ºå¤§çš„å›¾åƒèåˆä¸åŸºäºç›¸ä¼¼åº¦çš„æ™ºèƒ½æ•°æ®æ‰©å¢åŠŸèƒ½ã€‚

## ğŸ”¥ ä¸»è¦åŠŸèƒ½

### 1. DDPMå™ªå£°ç©ºé—´å›¾åƒèåˆ

æˆ‘ä»¬å®ç°äº†ä¸€ç§åœ¨DDPMå¯ç¼–è¾‘å™ªå£°ç©ºé—´ä¸­è¿›è¡Œå›¾åƒèåˆçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿç”Ÿæˆè¯­ä¹‰è¿è´¯çš„ç»“æœï¼ŒåŒæ—¶ä¿ç•™ä¸¤å¼ æºå›¾åƒçš„å…³é”®ç‰¹å¾ã€‚

![å›¾åƒèåˆç¤ºä¾‹](https://i.imgur.com/3MaQyJS.png)

**æ ¸å¿ƒç‰¹ç‚¹:**
- å››ç§èåˆæ¨¡å¼:
  - **çº¿æ€§èåˆ (Linear)**: ç®€å•çš„åŠ æƒæ··åˆ
  - **æ¸å˜èåˆ (Cross Fade)**: æ—¶é—´æ­¥ç›¸å…³çš„æ¸è¿›æ··åˆ
  - **ç‰¹å¾é€‰æ‹©æ€§èåˆ (Feature Selective)**: ä¿ç•™æ¯å¼ å›¾åƒä¸­çš„ä¸»è¦ç‰¹å¾
  - **å±‚é€‰æ‹©æ€§èåˆ (Layer Selective)**: å¯¹ä¸åŒå™ªå£°å±‚æ¬¡åº”ç”¨ä¸åŒçš„èåˆç­–ç•¥

**ä½¿ç”¨æ–¹æ³•:**
```bash
python main_run.py --mode="image_fusion" --dataset_yaml="fusion_examples.yaml" --second_image="path/to/second/image.jpg" --fusion_ratio=0.5 --fusion_mode="linear"
```

### 2. åŸºäºç›¸ä¼¼åº¦çš„æ™ºèƒ½æ•°æ®æ‰©å¢

æˆ‘ä»¬å¼€å‘äº†ä¸€å¥—æ™ºèƒ½æ•°æ®æ‰©å¢ç³»ç»Ÿï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹è®­ç»ƒé›†ä¸­çš„ç›¸ä¼¼å›¾åƒå¹¶èåˆå®ƒä»¬ï¼Œåˆ›é€ è¯­ä¹‰è¿è´¯çš„æ‰©å¢æ ·æœ¬ã€‚

**æ ¸å¿ƒç‰¹ç‚¹:**
- **è‡ªåŠ¨ç›¸ä¼¼åº¦æ£€æµ‹**ï¼šä½¿ç”¨ResNet50æå–å›¾åƒç‰¹å¾å¹¶è®¡ç®—ç›¸ä¼¼åº¦
- **æ™ºèƒ½èåˆ**ï¼šåªèåˆè¶³å¤Ÿç›¸ä¼¼çš„å›¾åƒï¼Œé¿å…ç”Ÿæˆæ··ä¹±ç»“æœ
- **æ‰¹é‡å¤„ç†**ï¼šé«˜æ•ˆå¤„ç†æ•´ä¸ªè®­ç»ƒé›†
- **å¤šç§èåˆç­–ç•¥**ï¼šæ”¯æŒå¤šç§èåˆæ¨¡å¼ï¼Œç”Ÿæˆå¤šæ ·åŒ–è¾“å‡º

**ä½¿ç”¨æ–¹æ³•:**
```bash
python augment_dataset_fusion.py --input_dir="training_images" --output_dir="augmented_dataset" --similarity_threshold=0.7
```

## ğŸ“š æ–‡æ¡£

é¡¹ç›®åŒ…å«ä»¥ä¸‹è¯¦ç»†æ–‡æ¡£:

- [IMAGE_FUSION_README.md](IMAGE_FUSION_README.md) - å›¾åƒèåˆåŠŸèƒ½è¯¦ç»†æŒ‡å—
- [FUSION_AUGMENTATION_README.md](FUSION_AUGMENTATION_README.md) - åŸºäºç›¸ä¼¼åº¦çš„æ•°æ®æ‰©å¢æŒ‡å—
- [PROJECT_CHANGES.md](PROJECT_CHANGES.md) - é¡¹ç›®ä¿®æ”¹å®Œæ•´è®°å½•

## ğŸ§  æŠ€æœ¯ç»†èŠ‚

æˆ‘ä»¬çš„å®ç°åˆ©ç”¨äº†DDPMå¯ç¼–è¾‘å™ªå£°ç©ºé—´çš„ç‹¬ç‰¹ç‰¹æ€§:

- **æ—¶é—´æ­¥ä¹‹é—´çš„ç»Ÿè®¡ç‹¬ç«‹æ€§**ï¼šå…è®¸åˆ†å±‚ç¼–è¾‘
- **å¯æ§çš„éšæœºæ€§**ï¼šèƒ½å¤Ÿç”Ÿæˆå¤šæ ·åŒ–è¾“å‡º
- **ç»“æ„ä¿ç•™**ï¼šç¡®ä¿èåˆç»“æœçš„è¯­ä¹‰è¿è´¯æ€§

ç›¸ä¼¼åº¦æ£€æµ‹ç³»ç»Ÿä½¿ç”¨é¢„è®­ç»ƒResNet50æ¨¡å‹æå–çš„ç‰¹å¾å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç¡®ä¿åªæœ‰çœŸæ­£ç›¸ä¼¼çš„å›¾åƒæ‰ä¼šè¢«èåˆã€‚

## ğŸ” ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•çš„ä¼˜åŠ¿

- **è¯­ä¹‰ç†è§£**ï¼šä¿ç•™æºå›¾åƒçš„è¯­ä¹‰ä¿¡æ¯
- **è‡ªç„¶è¿‡æ¸¡**ï¼šé¿å…åƒç´ çº§æ··åˆå¸¸è§çš„ä¼ªå½±
- **åˆ†å±‚æ§åˆ¶**ï¼šé€‰æ‹©æ€§åœ°èåˆä¸åŒç‰¹å¾å±‚æ¬¡
- **æ™ºèƒ½ç­›é€‰**ï¼šåªèåˆè¯­ä¹‰ç›¸ä¼¼çš„å›¾åƒ
- **å¤šæ ·åŒ–æ•ˆæœ**ï¼šå¤šç§èåˆæ¨¡å¼æ»¡è¶³ä¸åŒåˆ›æ„éœ€æ±‚

## ğŸ“‹ ç³»ç»Ÿè¦æ±‚

- Python 3.7+
- PyTorch 1.7+
- torchvision
- scikit-learn (ç”¨äºç›¸ä¼¼åº¦è®¡ç®—)
- PIL (Pillow)
- numpy
- tqdm
- PyYAML

## ğŸš€ å¿«é€Ÿå…¥é—¨

1. å…‹éš†æ­¤ä»“åº“
2. å®‰è£…ä¾èµ–é¡¹
3. è¿è¡Œä¸Šè¿°ç¤ºä¾‹å‘½ä»¤
4. æŸ¥é˜…æ–‡æ¡£äº†è§£è¯¦ç»†ä½¿ç”¨è¯´æ˜

## ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäºMITè®¸å¯è¯å¼€æº - è¯¦è§LICENSEæ–‡ä»¶